{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjPreprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O84mo4ysKWBQ",
        "outputId": "bad274b3-5424-4364-d2a5-bda5a34ec257"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install xmltodict"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjPE3oizKX9U",
        "outputId": "fc1ae757-46de-4a7f-da71-36797ffa4252"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import re\n",
        "\n",
        "polarity_dict = {'positive':'pos', 'negative':'neg', 'neutral':'neu', 'conflict':'con'}\n",
        "\n",
        "def get_labels(sentence, tokens, aspect_list):\n",
        "  if len(aspect_list) == 0:\n",
        "    return ['O' for i in range(len(tokens))]\n",
        "\n",
        "  sentence = sentence.lower()\n",
        "  new_tokens = tokens[1:-1]\n",
        "  cur_pos = 0\n",
        "  cur_asp_idx = 0\n",
        "  ans = ['O']\n",
        "  for x in new_tokens:\n",
        "    cur_sub_word = x if x[:2]!='##' else x[2:]\n",
        "    #pattern = re.compile(cur_sub_word)\n",
        "    #match = pattern.search(sentence, cur_pos)\n",
        "    #s,e = match.span()\n",
        "    s = sentence.find(cur_sub_word, cur_pos)\n",
        "    e = s + len(cur_sub_word)\n",
        "    if cur_asp_idx < len(aspect_list) and s >= aspect_list[cur_asp_idx]['start'] and s<aspect_list[cur_asp_idx]['end']:\n",
        "      cur_char = 'B' if ans[-1]=='O' else 'I'\n",
        "      ans.append(f'{cur_char}-{polarity_dict[aspect_list[cur_asp_idx][\"polarity\"]]}')\n",
        "    else:\n",
        "      ans.append('O')\n",
        "    if cur_asp_idx < len(aspect_list) and s >= aspect_list[cur_asp_idx]['end']:\n",
        "      cur_asp_idx += 1\n",
        "    cur_pos = e\n",
        "  ans.append('O')\n",
        "  return ans\n",
        "\n",
        "\n",
        "my_sentence = \"I charge it at night and skip taking the cord with me because of the good battery life.\"\n",
        "polarity = ['neutral', 'positive']\n",
        "start_of_aspect = [41, 74]\n",
        "end_of_aspect = [45, 86]\n",
        "aspect_list = [{'polarity':'neutral', 'start':41, 'end':45}, {'polarity':'positive', 'start':74, 'end':86}]\n",
        "\n",
        "print(len(my_sentence))\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = tokenizer.encode(my_sentence)\n",
        "subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(input_ids)\n",
        "print(subwords)\n",
        "labels = get_labels(my_sentence, subwords, aspect_list)\n",
        "print(labels)\n",
        "assert len(input_ids) == len(labels)\n",
        "\n",
        "# model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# a = 1/0\n",
        "# output = model(input_ids, type_ids, att_m)\n",
        "\n",
        "# final_layer = output.last_hidden_state\n",
        "# final_layer.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87\n",
            "[101, 1045, 3715, 2009, 2012, 2305, 1998, 13558, 2635, 1996, 11601, 2007, 2033, 2138, 1997, 1996, 2204, 6046, 2166, 1012, 102]\n",
            "['[CLS]', 'i', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', 'life', '.', '[SEP]']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-neu', 'O', 'O', 'O', 'O', 'O', 'O', 'B-pos', 'I-pos', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLI3YD14OChL",
        "outputId": "db05851a-96cb-4bb9-8f7d-22988bd3abb0"
      },
      "source": [
        "import xmltodict\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(\"Laptop_Train_v2.xml\", \"r\") as f:\n",
        "  obj = xmltodict.parse(f.read())\n",
        "\n",
        "obj = obj['sentences']['sentence']\n",
        "out_sid = []\n",
        "out_tokens = []\n",
        "out_labels = []\n",
        "for x in tqdm(obj, total=len(obj)):\n",
        "  sentence = x['text']\n",
        "  sent_id = x['@id']\n",
        "  aspect_list = []\n",
        "  if 'aspectTerms' in x.keys():\n",
        "    aspects = x['aspectTerms']['aspectTerm']\n",
        "    if type(aspects) != list:\n",
        "      aspects = [aspects]\n",
        "    for a in aspects:\n",
        "      aspect_list.append({'polarity':a['@polarity'], 'start':int(a['@from']), 'end':int(a['@to'])})\n",
        "    aspect_list.sort(key=lambda x: x['start'])\n",
        "  input_ids = tokenizer.encode(sentence)\n",
        "  subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "  \n",
        "  labels = get_labels(sentence, subwords, aspect_list)\n",
        "  # print(sentence)\n",
        "  # print(subwords)\n",
        "  # print(aspect_list)\n",
        "  # print(labels)\n",
        "  # print()\n",
        "  assert len(labels) == len(subwords)\n",
        "  out_sid.append(sent_id)\n",
        "  out_tokens.append(input_ids)\n",
        "  out_labels.append(labels)\n",
        "\n",
        "print('Writing to file...')\n",
        "df = pd.DataFrame()\n",
        "df['sid'] = out_sid\n",
        "df['token_ids'] = out_tokens\n",
        "df['labels'] = out_labels\n",
        "df.to_csv('preproc.csv', index=False)\n",
        "print('DONE')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3045/3045 [00:01<00:00, 1602.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing to file...\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}